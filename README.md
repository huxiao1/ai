# Model Compression for Efficient Object Detection
1. Experiment 1: Quantization-Based Model Compression for Object Detection

We employed the int8 quantization technique to simplify an AlexNet-based model, significantly reducing inference time on the GPU for object detection tasks. This approach effectively compressed the model without compromising its accuracy.

2. Experiment 2: Pruning-Based Model Compression for Real-Time Object Detection

We applied pruning techniques to simplify a YOLO-v3 model, achieving a substantial reduction in inference time on the GPU for real-time object detection. This method enabled the model to operate efficiently on resource-constrained devices without sacrificing its performance.
